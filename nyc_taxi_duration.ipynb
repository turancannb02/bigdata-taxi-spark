{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "495f6be6-c5be-4bcd-8b8e-ca2a29d7dc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# NYC Yellow-Taxi duration model — 50 % sample, 4 GB driver\n",
    "# -----------------------------------------------------------\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, unix_timestamp, hour, dayofweek\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fffdbe9-8af4-4afc-bd5f-15e6415f0138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark session with larger heap & fewer shuffle files\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"NYC Duration RF 50pct\")\n",
    "         .master(\"local[*]\")\n",
    "         .config(\"spark.driver.memory\", \"4g\")          #  ➜ 4 GB heap\n",
    "         .config(\"spark.sql.shuffle.partitions\", \"48\") #  ➜ fewer temp files\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e093a7d-40b0-4db3-9f73-10c74869be25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.shape  → (4,591,845, 20)\n",
      "\n",
      "--- df.info()  (via printSchema) ---\n",
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- Airport_fee: double (nullable = true)\n",
      " |-- cbd_congestion_fee: double (nullable = true)\n",
      "\n",
      "\n",
      "--- first 5 rows (via show) ---\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|Airport_fee|cbd_congestion_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+\n",
      "|1       |2025-05-01 00:07:06 |2025-05-01 00:24:15  |1              |3.7          |1         |N                 |140         |202         |1           |18.4       |4.25 |0.5    |4.85      |0.0         |1.0                  |29.0        |2.5                 |0.0        |0.75              |\n",
      "|2       |2025-05-01 00:07:44 |2025-05-01 00:14:27  |1              |1.03         |1         |N                 |234         |161         |1           |8.6        |1.0  |0.5    |4.3       |0.0         |1.0                  |18.65       |2.5                 |0.0        |0.75              |\n",
      "|2       |2025-05-01 00:15:56 |2025-05-01 00:23:53  |1              |1.57         |1         |N                 |161         |234         |2           |10.0       |1.0  |0.5    |0.0       |0.0         |1.0                  |15.75       |2.5                 |0.0        |0.75              |\n",
      "|2       |2025-05-01 00:00:09 |2025-05-01 00:25:29  |1              |9.48         |1         |N                 |138         |90          |1           |40.8       |6.0  |0.5    |11.7      |6.94        |1.0                  |71.94       |2.5                 |1.75       |0.75              |\n",
      "|2       |2025-05-01 00:45:07 |2025-05-01 00:52:45  |1              |1.8          |1         |N                 |90          |231         |1           |10.0       |1.0  |0.5    |1.5       |0.0         |1.0                  |17.25       |2.5                 |0.0        |0.75              |\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load parquet slice\n",
    "df = spark.read.parquet(\"data/nyc_yellow_2023-01.parquet\")\n",
    "\n",
    "row_cnt = df.count()\n",
    "col_cnt = len(df.columns)\n",
    "print(f\"df.shape  → ({row_cnt:,}, {col_cnt})\")       \n",
    "\n",
    "print(\"\\n--- df.info()  (via printSchema) ---\")\n",
    "df.printSchema()\n",
    "\n",
    "print(\"\\n--- first 5 rows (via show) ---\")\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f80b700c-de66-4452-9d75-ed7edfcd60d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature-engineering\n",
    "\n",
    "taxi = (df\n",
    "        .withColumn(\"duration_min\",\n",
    "                    (unix_timestamp(\"tpep_dropoff_datetime\") -\n",
    "                     unix_timestamp(\"tpep_pickup_datetime\")) / 60)\n",
    "        .filter((col(\"duration_min\") > 0) & (col(\"duration_min\") < 180))\n",
    "        .withColumn(\"hour\",  hour(\"tpep_pickup_datetime\"))\n",
    "        .withColumn(\"wday\",  dayofweek(\"tpep_pickup_datetime\"))\n",
    "        .select(\"passenger_count\", \"trip_distance\",\n",
    "                \"hour\", \"wday\", \"PULocationID\", \"DOLocationID\",\n",
    "                \"duration_min\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c114f09-3efc-4e1b-b606-dc2e7b4bdb9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rows after cleaning = 3,330,149\n",
      "Rows in 50 % sample  = 1,666,724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Drop remaining nulls + 50 % sample\n",
    "taxi_clean  = taxi.na.drop()\n",
    "taxi_sample = taxi_clean.sample(fraction=0.5, seed=42)\n",
    "\n",
    "print(f\"\\nRows after cleaning = {taxi_clean.count():,}\")\n",
    "print(f\"Rows in 50 % sample  = {taxi_sample.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e70ead7-aea8-491c-8a0c-0ba6ea978cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/25 23:48:20 WARN DAGScheduler: Broadcasting large task binary with size 1022.8 KiB\n",
      "25/06/25 23:48:35 WARN DAGScheduler: Broadcasting large task binary with size 1978.8 KiB\n",
      "25/06/25 23:48:52 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n",
      "25/06/25 23:49:11 WARN DAGScheduler: Broadcasting large task binary with size 1205.5 KiB\n",
      "25/06/25 23:49:13 WARN DAGScheduler: Broadcasting large task binary with size 7.2 MiB\n",
      "25/06/25 23:49:43 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "25/06/25 23:49:45 WARN DAGScheduler: Broadcasting large task binary with size 13.7 MiB\n",
      "25/06/25 23:50:25 WARN DAGScheduler: Broadcasting large task binary with size 4.2 MiB\n",
      "25/06/25 23:50:29 WARN DAGScheduler: Broadcasting large task binary with size 25.6 MiB\n",
      "25/06/25 23:51:22 WARN DAGScheduler: Broadcasting large task binary with size 7.7 MiB\n",
      "WARNING: An illegal reflective access operation has occurred                    \n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/home/tur24958/bigdata-lab/venv/lib/python3.12/site-packages/pyspark/jars/spark-core_2.12-3.5.0.jar) to field java.nio.charset.Charset.name\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    }
   ],
   "source": [
    "# ML pipeline\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"passenger_count\",\"trip_distance\",\"hour\",\n",
    "               \"wday\",\"PULocationID\",\"DOLocationID\"],\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"skip\")\n",
    "\n",
    "rf = RandomForestRegressor(\n",
    "        labelCol=\"duration_min\",\n",
    "        featuresCol=\"features\",\n",
    "        numTrees=50,           # lighter memory than 100\n",
    "        maxDepth=12,\n",
    "        seed=42)\n",
    "\n",
    "model  = Pipeline(stages=[assembler, rf]).fit(taxi_sample)\n",
    "preds  = model.transform(taxi_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d743f18b-0cc6-457d-8632-6d459d6abc93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Model metrics on 50 % sample ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 6.715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE : 4.116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 53:==================================================>       (7 + 1) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R²  : 0.807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Metrics\n",
    "\n",
    "ev = RegressionEvaluator(labelCol=\"duration_min\", predictionCol=\"prediction\")\n",
    "print(\"\\n--- Model metrics on 50 % sample ---\")\n",
    "print(\"RMSE:\", round(ev.setMetricName(\"rmse\").evaluate(preds), 3))\n",
    "print(\"MAE :\", round(ev.setMetricName(\"mae\").evaluate(preds), 3))\n",
    "print(\"R²  :\", round(ev.setMetricName(\"r2\").evaluate(preds), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8bd39b-3fea-414e-8d2f-2b9b36883848",
   "metadata": {},
   "source": [
    "### Results (50 % sample, 50-tree RF)\n",
    "\n",
    "* **RMSE**: 6.72 min  \n",
    "* **MAE** : 4.12 min  \n",
    "* **R²**  : 0.81\n",
    "\n",
    "Reducing the dataset to 50 % and the forest size to 50 trees lowers\n",
    "memory footprint from >4 GB to <3 GB, at the cost of ~3 min higher RMSE.\n",
    "The model still explains 81 % of variance, sufficient for a coursework\n",
    "demonstration of Spark-based trip-duration prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d57ec48-a585-43b2-b770-52d01e8d2696",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
